from . import *
from utils.util import google_search_scrape, extract_content

class State(MessagesState):
    is_search: str
    main_context : str
    suffix_context : str
    
class ChatbotAgent:
    def __init__(self):
        self.LIMIT_LENGTH = 10
        self.system_prompt = "ë‹¹ì‹ ì˜ ì´ë¦„ì€ 'ë¯¸ë„¤ë¥´ë°”'ì´ê³  ì¹´ì¹´ì˜¤í†¡ì—ì„œ í™œë™í•˜ëŠ” ì±—ë´‡ì…ë‹ˆë‹¤. 'ccw'ë‹˜ì´ ê´€ë¦¬í•˜ê³  ìˆëŠ” ì±—ë´‡ì…ë‹ˆë‹¤. ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ ìš”ì²­ì— ì¹œì ˆí•˜ê²Œ ì‘ë‹µí•©ë‹ˆë‹¤. ê°€ëŠ¥í•œ í•µì‹¬ì ì¸ ë‚´ìš©ë§Œì„ ì „ë‹¬í•˜ì„¸ìš”. ì›ƒëŠ” ì´ëª¨ì§€ë¥¼ ì‚¬ìš©í•˜ì—¬ ì¹œì ˆí•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.\n"
        self.llm = ChatOpenAI(model="gpt-4o")
        self.config = {"configurable": {"thread_id": "default",
                                        "user_id": "default"}}
        self._build_graph()
        
    async def get_gpt_response(self,
                               question: str) -> str:
        if question == "ìƒˆë¡œìš´ ëŒ€í™”":
            self._build_graph()
        question = HumanMessage(content="ì•ˆë…•í•˜ì„¸ìš”~")
        return self._call_graph([question])["messages"][-1].content
    
    def set_config(self,
                   user_id:str):
        self.config = {"configurable": {"thread_id": user_id,
                                        "user_id": user_id}}
        
    def _build_graph(self):
        """
            Des:
                ê·¸ë˜í”„ ìƒì„±í•¨ìˆ˜
        """
        builder = StateGraph(State)
        builder.add_node("_Node_answer", self._Node_answer)
        builder.add_node("_Node_write_memory", self._Node_write_memory)
        builder.add_node("_Node_optimize_memory", self._Node_optimize_memory)
        builder.add_node("_Node_decide_search", self._Node_decide_search)
        builder.add_node("_Node_search", self._Node_search)
        
        builder.add_edge(START, "_Node_decide_search")
        builder.add_conditional_edges("_Node_decide_search", self._decide_search)
        builder.add_edge("_Node_search", "_Node_answer")
        builder.add_edge("_Node_answer", "_Node_write_memory")
        builder.add_conditional_edges("_Node_write_memory", self._check_memory_length)
        builder.add_edge("_Node_optimize_memory", END)
        self.graph = builder.compile(checkpointer=ShortTermMemory,
                                      store=LongTermMemory)

    @trace_function(enable_print=False, only_node=True)
    def _Node_answer(self, 
                    state: State, 
                    config: RunnableConfig,
                    store: BaseStore):
        """
            Des:
                ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ì¸ì‹í•˜ê³ , ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ë…¸ë“œ
        """
        # ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì§€ì •
        namespace = ("memories", config["configurable"]["user_id"])
        key = "chat_user_memory"
        memory = self._get_memory(namespace=namespace, 
                            key=key, 
                            store=store)
        system_message = prompt_config.answer_prompt.format(memory=memory)
        
        # context í™•ì¸ ë° ë‹µë³€ ìƒì„±
        if state.get("is_search") == "YES":
            prompt = prompt_config.answer_with_context.format(context=state["main_context"],
                                                              query=state['messages'][-1].content)
            answer = self.llm.invoke(prompt).content + "\n" + state.get("suffix_context")
            return {"messages": [AIMessage(content=answer)]}
        else:    
            prompt = [SystemMessage(content=self.system_prompt+system_message)] + state["messages"]
            # print(f"{PINK}\n{prompt[0].content}\n{RESET}")
            response = self.llm.invoke(prompt)
            return {"messages": response}

    @trace_function(enable_print=False, only_node=True)
    def _Node_write_memory(self,
                          state: State, 
                          config: RunnableConfig, 
                          store: BaseStore):
        """
            Des:
                ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ì¸ì‹í•˜ê³ , ê°œì¸ì •ë³´ë¡œ ì €ì¥í•˜ëŠ” ë…¸ë“œ
        """
        namespace = ("memories", config["configurable"]["user_id"])
        key = "chat_user_memory"
        memory = self._get_memory(namespace=namespace, 
                                  key=key, 
                                  store=store)
        system_message = prompt_config.create_memory_prompt.format(memory=memory)
        prompt = [SystemMessage(content=system_message)]+state["messages"]
        response = self.llm.invoke(prompt)
        store.put(namespace=namespace, 
                key=key, 
                value={"memory":response.content})
        # print(f"{RED}\ní˜„ì¬ STATE ê°œìˆ˜: {len(state['messages'])}\n{RESET}")
    
    @trace_function(enable_print=False, only_node=True)
    def _Node_optimize_memory(self,
                              state: State):
        """
            Des:
                ë©”ëª¨ë¦¬ ìµœì í™” í•¨ìˆ˜
        """
        delete_messages = [RemoveMessage(id=m.id) for m in state["messages"][:-self.LIMIT_LENGTH]]
        return {"messages": delete_messages}

    @trace_function(enable_print=False, only_node=True)
    def _Node_decide_search(self,
                           state: State):
        system_message = "í˜„ì¬ ì‚¬ìš©ì ìš”ì²­ë¬¸ì´ ë‰´ìŠ¤ ê²€ìƒ‰ì´ í•„ìš”í•œì§€ íŒë‹¨í•˜ì„¸ìš”. ë‹µë³€ì€ ë¬´ì¡°ê±´ YES ë˜ëŠ” NOë¡œ ì¶œë ¥í•˜ì„¸ìš”."
        return {"is_search": [self.llm.invoke([SystemMessage(content=system_message)] + state["messages"])][0].content.upper()}

    @trace_function(enable_print=False, only_node=True)
    def _Node_search(self,
                    state: State):
        query = state['messages'][-1].content # TODO humanmessage ì¸ì§€ ì²´í¬í•„ìš”
        prompt = prompt_config.generate_search_info.format(query=query)
        search_info = self.llm.invoke(prompt).content
        results = google_search_scrape(search_info, num_results=3)
        print(f"{RED}ê²€ìƒ‰ì–´ : {search_info}{RESET}")
        print(f"{RED}ê²€ìƒ‰ê²°ê³¼ : {len(results)}{RESET}")
        # TODO ê²°ê³¼ì—†ì„ë•Œ ì²˜ë¦¬í•„ìš”
        main_context = ''
        suffix_context = ''
        for idx, result in enumerate(results):
            link = result.get("link")
            desc, detailed_content = extract_content(link)
            main_context += f"ì œëª© : {result.get('title')}\në§í¬ : {link}\nì„¤ëª… : {desc}\në‚´ìš© : {detailed_content}\n\n"    
            suffix_context += f"""
ğŸ“Œ ì°¸ê³ ë‚´ìš© [{idx+1}]
ì œëª© : {result.get('title')}
ë§í¬ : {link}
ì„¤ëª… : {desc}
"""
        return {"main_context": main_context, "suffix_context": suffix_context}


    def _check_memory_length(self,
                             state: State):
        if len(state["messages"]) > self.LIMIT_LENGTH:
            return "_Node_optimize_memory"
        else:
            return END
    
    def _get_memory(self,
                    namespace, 
                    key,
                    store:BaseStore):
        """
            Des:
                í˜„ì¬ ì €ì¥ëœ ì‚¬ìš©ì ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
        """
        existing_memory = store.get(namespace=namespace,
                                    key=key)
        return existing_memory.value.get('memory') if existing_memory else ""

    def _call_graph(self, 
                    messages):
        return self.graph.invoke({"messages": messages}, 
                                 config=self.config)

    
    def _decide_search(self,
                       state: State):
        if "YES" in state["is_search"]:
            return "_Node_search"
        else:
            return "_Node_answer"