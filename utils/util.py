import os
from functools import wraps
from dotenv import load_dotenv
from urllib.parse import urlparse, urlunparse, urljoin
from langchain_community.document_loaders import AsyncHtmlLoader
from langchain_community.document_transformers import Html2TextTransformer
import requests
from bs4 import BeautifulSoup

RESET = "\033[0m"        # Reset to default
RED = "\033[91m"         # Bright Red
BLUE = "\033[94m"        # Bright Blue
GREEN = "\033[92m"        # Bright Green
YELLOW = "\033[93m"       # Bright Yellow
PINK = "\033[95m"         # Bright Pink

def set_env():
    load_dotenv()
    os.environ["LANGCHAIN_API_KEY"] = os.getenv("LANGCHAIN_API_KEY")
    os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
    os.environ["LANGCHAIN_TRACING_V2"] = "true"
    os.environ["LANGCHAIN_PROJECT"] = "langchain-academy"

def trace_function(enable_print=True, only_node=False):
    def wrapper(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            if enable_print:
                if only_node:
                    print(f"\n{GREEN}ğŸš€ Passing Through [{func.__name__}] ..{RESET}")
                else:
                    print(f"\n{GREEN}ğŸš€ Passing Through [{func.__name__}] ..{RESET}")
                    print(f"\n{RED}#### [Input State]{RESET}")
                    print(f"  args: {args}")
                    print(f"  kwargs: {kwargs}")
            result = func(*args, **kwargs)
            if enable_print:
                if only_node:
                    pass
                else:
                    print(f"\n{BLUE}#### [Output State]{RESET}")
                    print(f"  result: {result}")
            return result
        return wrapper
    return wrapper
    
def extract_content(link:str) -> tuple[str, str]:
    """
        Des:
            ì£¼ì–´ì§„ ë§í¬ì— ëŒ€í•œ ë‚´ìš© ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜
        Args:
            link (str): ì¶”ì¶œí•  ë§í¬
        Returns:
            tuple[str, str]: ì¶”ì¶œëœ ë‚´ìš©ì„ ë‹´ì€ íŠœí”Œ
    """
    loader = AsyncHtmlLoader(link)
    docs = loader.load()
    html2text = Html2TextTransformer()
    docs_transformed = html2text.transform_documents(docs,metadata_type="html")
    desc = docs_transformed[0].metadata.get('description',"")
    detailed_content = docs_transformed[0].page_content
    return desc,detailed_content

def google_search_scrape(query:str, 
                         num_results:int=3) -> list:
    """
        Des:
            Google ê²€ìƒ‰ ê²°ê³¼ë¥¼ ìŠ¤í¬ë˜í•‘í•˜ëŠ” í•¨ìˆ˜
        Args:
            query (str): ê²€ìƒ‰í•  í‚¤ì›Œë“œ
            num_results (int): ê²€ìƒ‰ ê²°ê³¼ ìˆ˜
        Returns:
            list: ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë‹´ì€ ë¦¬ìŠ¤íŠ¸
    """
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36"
    }
    query = query.replace(" ", "+")
    url = f"https://www.google.com/search?q={query}&num={num_results}"
    response = requests.get(url, headers=headers)
    soup = BeautifulSoup(response.text, "html.parser")
    results = []
    for g in soup.find_all("div", class_="tF2Cxc"):
        title = g.find("h3").text.upper()
        link = g.find("a")["href"]
        results.append({"title": title, "link": link})
    return results